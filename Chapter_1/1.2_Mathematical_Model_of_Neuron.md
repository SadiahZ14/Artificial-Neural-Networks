# Mathematical Model of a neuron:

A neuron can be modeled as a function  $f$  that takes a vector of inputs  $\mathbf{x}$  and produces an output  $\mathbf{y}$ . <br>
The function  $f$ consists of two main parts: 
 - The linear combination of inputs and weights, and 
 - The application of an activation function. 
 
Hereâ€™s the step-by-step breakdown:

### 1. Linear Combination (Affine Transformation)

This is the first step where the inputs are combined with the corresponding weights and a bias is added.

- **Equation**: 
$z = \mathbf{w}^\top \mathbf{x} + b$

- **Variables**:
    - $ \mathbf{x} $ is the vector of inputs $[x_1, x_2, \ldots, x_n]$,
    - $ \mathbf{w} $ is the vector of weights $[w_1, w_2, \ldots, w_n]$,
    - $ b $ is the bias,
    - $ z $ is the weighted sum.

The multiplication of the weights and inputs can be expanded as:

$z = w_1 \cdot x_1 + w_2 \cdot x_2 + \ldots + w_n \cdot x_n + b$

This represents the dot product of the weights vector $ \mathbf{w} $ with the inputs vector $ \mathbf{x} $, followed by adding the bias $ b $.

### 2. Activation Function

The weighted sum $z$ is then passed through an activation function $g$ to introduce non-linearity and make complex pattern learning possible.

- **Activation Function Application**:  

  $\mathbf{y} = g(z)$

  where
  - $g$ is the activation function (e.g., sigmoid, tanh, ReLU),
  - $\mathbf{y}$ is the output of the neuron.

### Combined Neuron Function

Combining the above components, the function of a neuron $f$ can be described as:

$f(\mathbf{x}) = g(\mathbf{w}^\top \mathbf{x} + b)$

This function encapsulates the entire operation of a neuron in an ANN, from input to output. It highlights how each neuron processes its inputs to produce an output that is dependent on both the learned parameters (weights and biases) and the chosen non-linear transformation (activation function).

![alt text](images/rotation_sine_distortion.gif)
