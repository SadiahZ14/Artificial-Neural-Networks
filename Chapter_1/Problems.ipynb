{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Basic Binary Classification Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Construct a single neuron to determine if a number is positive or negative.\n",
    "- **Inputs**: Single feature, a real number \\( x \\).\n",
    "- **Weights and Bias**: Initialize weight \\( w = 1.0 \\) and bias \\( b = 0 \\).\n",
    "- **Activation Function**: Use the sigmoid function to output a probability.\n",
    "- **Task**: Write the mathematical model of this neuron and manually calculate the output for \\( x = 2 \\) and \\( x = -1 \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for x = 2 is  0.8807970779778823\n",
      "Output for x = -1 is  0.2689414213699951\n"
     ]
    }
   ],
   "source": [
    "# Solution for Problem 1:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "w = 1 # weight\n",
    "b = 0 # bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Mathematical model of a neuron\n",
    "def neuron(x):\n",
    "    w_t= np.transpose(w)\n",
    "    y = sigmoid(w_t * x + b)\n",
    "    return y\n",
    "\n",
    "print(f\"Output for x = 2 is \", neuron(2))\n",
    "print(f\"Output for x = -1 is \", neuron(-1))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 2: Comparing Activation Functions with a Pattern Analysis\n",
    "**Task**: Investigate how three different activation functions respond to a range of inputs, simulating the behavior of neurons as light bulbs controlled by smart switches.\n",
    "\n",
    "- **Setup**:\n",
    "  - **Inputs**: Use a series of inputs ranging from -2.0 to 2.0 in increments of 0.5 to simulate varying electrical currents.\n",
    "  - **Weights and Bias**: Assume a weight \\( w = 1.0 \\) and a bias \\( b = 0.5 \\), which together determine the net input to the neuron.\n",
    "\n",
    "- **Activation Functions**: Analyze the outputs using sigmoid, tanh, and ReLU functions.\n",
    "  - **Sigmoid Function**: Simulates a precise dimmer switch that controls the brightness subtly, preventing the light from ever being fully off or fully bright, but adjusting smoothly.\n",
    "  - **Tanh Function**: Acts like a dimmer that can also go negative (dim to dark), representing zero output as complete darkness.\n",
    "  - **ReLU Function**: Represents a simple on/off switch, turning the light on only if the input (after weight and bias adjustment) is positive.\n",
    "\n",
    "- **Task**:\n",
    "  - **Calculation**: For each input in the series, calculate the output using each of the three activation functions.\n",
    "  - **Visualization**: Plot the resulting outputs for each activation function on the same graph to visually compare how the light bulb's brightness changes across the input range.\n",
    "  - **Discussion**: Interpret the plots to describe how each activation function would control the \"firing\" of a neuron (light bulb). Discuss the practical implications of these differences in handling signals in a neural network.\n",
    "\n",
    "This exercise aims to vividly illustrate how different neural \"control mechanisms\" (activation functions) interpret and respond to the same range of inputs by varying the simulated \"light intensity\" of neurons. The comparison should help deduce patterns and understand the suitability of each function for different types of tasks in neural networks.\n",
    "\n",
    "### Problem 3: Output Behavior for Different Inputs\n",
    "**Task**: Model a neuron that predicts whether an object's size is above or below a threshold.\n",
    "- **Inputs**: Single feature representing size (e.g., \\( x = 10 \\)).\n",
    "- **Weights and Bias**: Assume \\( w = 0.1 \\) and \\( b = -1 \\).\n",
    "- **Activation Function**: Use the sigmoid function to model the output.\n",
    "- **Task**: Describe how the output of the neuron changes as the size value varies from 5 to 15.\n",
    "\n",
    "### Problem 4: Impact of Bias in Activation\n",
    "**Task**: Explore the impact of varying the bias on a neuron's output with a fixed input.\n",
    "- **Inputs**: Assume \\( x = 2.0 \\).\n",
    "- **Weights and Bias**: Use \\( w = 1.5 \\) and experiment with different bias values \\( b = -1, 0, 1 \\).\n",
    "- **Activation Function**: Use the ReLU function.\n",
    "- **Task**: Calculate and compare the outputs for each bias setting, discussing the role of bias in modulating the neuron's activation.\n",
    "\n",
    "These problems help you understand how single neurons function, focusing on the basics of input processing, weight and bias effects, and how activation functions shape the neuron's output. They provide a practical introduction to neural network fundamentals without delving into more complex network architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
